{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: SUPPORT VECTOR MACHINES\n",
    "\n",
    "Course 2023/24: *M. Caligiuri*, *P. Talli*, *F. Lincetto*, *F. Chiariotti*, *P. Zanuttigh*\n",
    "\n",
    "The notebook contains some simple tasks to be performed with **SUPPORT VECTOR MACHINES (SVM)**.\n",
    "\n",
    "Complete all the **required code sections** and **answer to all the questions**.\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "\n",
    "1. **Read carefully all cells** and **follow the instructions**.\n",
    "2. **Re-run all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "3. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "4. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command**.\n",
    "5. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Classification with Support Vector Machines\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVM) for weather classification. We will use a dataset collected using the Luxottica **iSee** glasses. These devices provide multiple **sensors mounted inside the glasses**, which can be accessed through a bluetooth connection.\n",
    "\n",
    "![I-SEE Glasses](data/isee.png \"I-SEE\")\n",
    "\n",
    "The dataset corresponds to 8 hours of atmospherical data recordings sampled every 3 seconds.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| ID  | Label       |\n",
    "| :-: | :-:         |\n",
    "| 0   | Sunny       |\n",
    "| 1   | Rain        |\n",
    "| 2   | Cloudy      |\n",
    "| 3   | Mostly Clear|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary step\n",
    "\n",
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB02.ipynb**, failure to do so will incur in a **lower grade**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Pietro Bernardi\n",
    "\n",
    "**ID Number**: 2097494"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the heplper functions\n",
    "\n",
    "In this section you will find some helper functions (some already implemented, some to be implemented by you) that will be used in the following sections.\n",
    "1. `load_dataset` -> to load the dataset from the file `data/lux.npz`,\n",
    "2. `plot_input` -> to plot the input data,\n",
    "3. `k_split` ->  to split the trainig dataset in k different folds,\n",
    "4. `k_fold_cross_validation` -> to perform the k-fold cross validation.\n",
    "\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the dataset\n",
    "def load_dataset(path: str) -> (np.ndarray, np.ndarray):\n",
    "    with np.load(path) as data:\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        \n",
    "        # Normalize the data\n",
    "        x -= x.mean(axis=0)\n",
    "        x /= x.std(axis=0)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix: np.ndarray, labels: np.ndarray) -> None:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    cmap = plt.cm.get_cmap('Accent', 4)\n",
    "    im = ax.scatter(X_matrix[:,0], X_matrix[:,1], X_matrix[:,2], c=labels, cmap=cmap)\n",
    "    im.set_clim(-0.5, 3.5)\n",
    "    cbar=fig.colorbar(im, ticks=[0,1,2,3], orientation='vertical', cmap=cmap)\n",
    "    cbar.ax.set_yticklabels(['Sunny', 'Rainy','Cloudy', 'Mostly clear']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset in k different folds\n",
    "def k_split(x: np.ndarray, y:np.ndarray, k: int, shuffle: bool = True) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    # Create a list of indices\n",
    "    idx = np.arange(x.shape[0])\n",
    "    # Shuffle the dataset\n",
    "    if shuffle:\n",
    "        # Randomly shuffle the indices\n",
    "        np.random.shuffle(idx)\n",
    "        # Shuffle the dataset\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    # Split the dataset in k folds\n",
    "    # ADD YOUR CODE HERE\n",
    "    # if k does not exactly divide the dataset's numerosity, the first (idx_shape[0]%k) folds will have an\n",
    "    # additional element.\n",
    "    fold_sizes = np.full(k, idx.shape[0]//k, dtype=int)\n",
    "    fold_sizes[:(idx.shape[0]%k)] += 1\n",
    "    # computing the splits\n",
    "    i = 0\n",
    "    x_folds, y_folds = [], []\n",
    "    for fs in fold_sizes:\n",
    "        # getting the data's indexes for this fold\n",
    "        i_upper = i + fs\n",
    "        idxs_fold = idx[i:i_upper]\n",
    "        # extracting the data\n",
    "        x_folds.append(x[idxs_fold])\n",
    "        y_folds.append(y[idxs_fold])\n",
    "        # updating the lower index\n",
    "        i = i_upper\n",
    "    \n",
    "    # returning the tuple\n",
    "    return tuple((x_folds, y_folds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the k-fold cross validation\n",
    "def k_fold_cross_validation(x_train: np.ndarray, y_train: np.ndarray, k: int, model: SVC, parameters: dict) -> tuple[tuple, tuple]:\n",
    "    # Define the folds for the cross validation\n",
    "    x_folds, y_folds = k_split(x_train, y_train, k)\n",
    "\n",
    "    # Build a list containing all of the possible combinatioon of parameters\n",
    "    params = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    # Initialize the dictionary of results\n",
    "    results = {k: 0 for k in params}\n",
    "\n",
    "    # For each param combination, perform the SVM training and testing\n",
    "    for param in params:\n",
    "        param = dict(zip(parameters.keys(), param))\n",
    "\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        # ADD YOUR CODE HERE\n",
    "        # setting the model parameters\n",
    "        model.set_params(**param)\n",
    "        \n",
    "        # for a given set of parameters\n",
    "        # we have 'k' different splits where k-1 folds are used collectively as training set and the remaining one\n",
    "        # is used as a test set in order to compute the fold accuracy\n",
    "        for test_idx in range(k):\n",
    "            # getting the indexes to index the training folds\n",
    "            train_idxs = np.array([train_idx for train_idx in range(k) if train_idx != test_idx])\n",
    "            # building the training and test sets for this split\n",
    "            X_training_set = np.concatenate(np.array(x_folds)[train_idxs])\n",
    "            X_test_set = x_folds[test_idx]\n",
    "            y_training_set = np.concatenate(np.array(y_folds)[train_idxs])\n",
    "            y_test_set = y_folds[test_idx]\n",
    "            # fitting the model with the given set of parameters on this particular split\n",
    "            model.fit(X_training_set, y_training_set)\n",
    "            # scoring the model for accuracy\n",
    "            fold_accuracies.append(model.score(X_test_set, y_test_set))\n",
    "        \n",
    "        # Compute the mean accuracy\n",
    "        results[tuple(param.values())] = round(np.mean(fold_accuracies), 4)\n",
    "    \n",
    "    # Find the best parameters\n",
    "    best_parameters = dict(zip(parameters.keys(), params[np.argmax(list(results.values()))]))\n",
    "    best_accuracy = np.max(list(results.values()))\n",
    "    best = (best_parameters, best_accuracy)\n",
    "\n",
    "    # Add the param name to the results\n",
    "    results = [({k: v for k, v in zip(parameters.keys(), p)}, a) for p, a in results.items()]\n",
    "\n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Hyper-parameter search\n",
    "\n",
    "### TO DO (A.0)\n",
    "\n",
    "**Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: 1234567 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "# as usual you can try different seeds by adding a constant to the number:\n",
    "# ID = 1234567 + X\n",
    "ID = 2097494# YOUR ID\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset and split it** in training and test set (while the **training** set is **typically larger**, here we set the number of training samples to 1000 and 4000 for the test data).\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the helper function\n",
    "X, y = load_dataset(\"data/lux.npz\")\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task is quite easy, let's add noise to make it more challenging!\n",
    "# You can try without noise (comment the next 2 lines, easy task), with the suggested amount of noise,\n",
    "# or play with the suggested amount of noise \n",
    "\n",
    "noise = np.random.normal(0, 0.1, X.shape)\n",
    "X = X + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.1)\n",
    "\n",
    "**Divide** the **data into training and test set** (for this part use 1000 samples in the **first** set, 4000 in the **second** one). Make sure that each label is present at least 10 times in training. If it is not, then keep adding permutations to the initial data until this happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permute the data and split into training and test taking the first 1000\n",
    "# data samples as training and 4000 samples as test\n",
    "permutation = np.random.permutation(X.shape[0]) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "X = X[permutation] # ADD YOUR CODE HERE (replace None)\n",
    "y = y[permutation] # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "m_training = 1000\n",
    "m_test = 4000\n",
    "\n",
    "X_train = X[:m_training] # ADD YOUR CODE HERE (replace None)\n",
    "X_test = X[m_training:(m_training+m_test)] # ADD YOUR CODE HERE (replace None)\n",
    "y_train = y[:m_training] # ADD YOUR CODE HERE (replace None)\n",
    "y_test = y[m_training:(m_training+m_test)] # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# checking if each label appears at least 10 times in training\n",
    "perm_idx = m_training + m_test\n",
    "l_freqs = np.unique(y_train, return_counts=True)[1]\n",
    "while (np.sum(l_freqs >= 10))!=len(l_freqs):\n",
    "    # need to add more permutations to the training dataset\n",
    "    X_train = np.concatenate([X_train, [X[perm_idx]]])\n",
    "    y_train = np.concatenate([y_train, [y[perm_idx]]])\n",
    "    # re-computing\n",
    "    l_freqs = np.unique(y_train, return_counts=True)[1]\n",
    "    \n",
    "print(\"X_train shape:\", X_train.shape,\"X_test shape:\", X_test.shape,\"||\",\"y_train shape:\",  y_train.shape,\"y_test shape:\", y_test.shape)\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True) # ADD YOUR CODE HERE. Hint: use np.unique() (replace None)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the plotting function\n",
    "plot_input(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.2)\n",
    "\n",
    "Use a SVM classfier with cross validation to pick a model. Use a 4-fold cross-validation. Let's start with a Linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [ 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Define the model (without parameters)\n",
    "svm = SVC(kernel='linear') # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, svm, parameters) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL')\n",
    "print(\"=\"*25)\n",
    "print(\"Best parameter set found:\")\n",
    "print(\"-\"*25)\n",
    "for k in best[0].keys():\n",
    "    print(f\"{k}\\t%2.2f\"%(best[0][k]))\n",
    "print(\"-\"*25)\n",
    "print(\"Score with best parameter: %1.4f\"%(best[1]))\n",
    "print()\n",
    "print(\"=\"*23)\n",
    "print(\"All scores on the grid:\")\n",
    "print(\"-\"*23)\n",
    "out = ''\n",
    "for pname in parameters.keys():\n",
    "    out += pname+'\\t'\n",
    "out += 'SCORE\\n'\n",
    "for r in results:\n",
    "    for pname in r[0]:\n",
    "        out += (\"%4.2f\"%(r[0][pname]))+'\\t'\n",
    "    out += (\"%1.4f\"%(r[1]))+'\\n\\r'\n",
    "print(out)\n",
    "\n",
    "# saving the best results for later\n",
    "best_params = {}\n",
    "best_params['linear'] = best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.3)\n",
    "\n",
    "Pick a model for the Polynomial kernel with degree=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "# Define an SVM with poly of degree 2 kernel (without parameters)\n",
    "poly2_svm = SVC(kernel='poly', degree=2) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, poly2_svm, parameters) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=2 KERNEL')\n",
    "print(\"=\"*32)\n",
    "print(\"Best parameter set found:\")\n",
    "print(\"-\"*32)\n",
    "for k in best[0].keys():\n",
    "    print(f\"{k}\\t%2.2f\"%(best[0][k]))\n",
    "print(\"-\"*32)\n",
    "\n",
    "print(\"Score with best parameter: %1.4f\"%(best[1]))\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"=\"*23)\n",
    "print(\"All scores on the grid:\")\n",
    "print(\"-\"*23)\n",
    "out = ''\n",
    "for pname in parameters.keys():\n",
    "    out += pname+'\\t'\n",
    "out += 'SCORE\\n'\n",
    "for r in results:\n",
    "    for pname in r[0]:\n",
    "        out += (\"%4.2f\"%(r[0][pname]))+'\\t'\n",
    "    out += (\"%1.4f\"%(r[1]))+'\\n\\r'\n",
    "print(out)\n",
    "\n",
    "# saving the best results for later\n",
    "best_params['poly2'] = best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.4)\n",
    "\n",
    "Now let's try a higher degree for the polynomial kernel (e.g., 3rd degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for poly with higher degree kernel\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01, 0.1, 1]}\n",
    "\n",
    "# Define an SVM with poly of higher degree kernel (without parameters)\n",
    "degree = 3\n",
    "poly_svm = SVC(kernel='poly', degree=degree) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, poly_svm, parameters) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print (f\"RESULTS FOR POLY DEGREE={degree} KERNEL\")\n",
    "\n",
    "print(\"=\"*32)\n",
    "print(\"Best parameter set found:\")\n",
    "print(\"-\"*32)\n",
    "for k in best[0].keys():\n",
    "    print(f\"{k}\\t%2.2f\"%(best[0][k]))\n",
    "print(\"-\"*32)\n",
    "\n",
    "print(\"Score with best parameter: %1.4f\"%(best[1]))\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"=\"*23)\n",
    "print(\"All scores on the grid:\")\n",
    "print(\"-\"*23)\n",
    "out = ''\n",
    "for pname in parameters.keys():\n",
    "    out += pname+'\\t'\n",
    "out += 'SCORE\\n'\n",
    "for r in results:\n",
    "    for pname in r[0]:\n",
    "        out += (\"%4.2f\"%(r[0][pname]))+'\\t'\n",
    "    out += (\"%1.4f\"%(r[1]))+'\\n\\r'\n",
    "print(out)\n",
    "\n",
    "# saving the best results for later\n",
    "best_params['poly3'] = best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.5)\n",
    "\n",
    "Pick a model for the Radial Basis Function kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for rbf SVM\n",
    "parameters = {'C': [0.1, 1, 10, 100],'gamma':[0.001, 0.01, 0.1,1]}\n",
    "\n",
    "# Define an SVM with rbf kernel (without parameters)\n",
    "rbf_svm = SVC(kernel='rbf') # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = k_fold_cross_validation(X_train, y_train, 4, rbf_svm, parameters) # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR rbf KERNEL')\n",
    "\n",
    "print(\"=\"*32)\n",
    "print(\"Best parameter set found:\")\n",
    "print(\"-\"*32)\n",
    "for k in best[0].keys():\n",
    "    print(f\"{k}\\t%2.2f\"%(best[0][k]))\n",
    "print(\"-\"*32)\n",
    "\n",
    "print(\"Score with best parameter: %1.4f\"%(best[1]))\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"=\"*23)\n",
    "print(\"All scores on the grid:\")\n",
    "print(\"-\"*23)\n",
    "out = ''\n",
    "for pname in parameters.keys():\n",
    "    out += pname+'\\t'\n",
    "out += 'SCORE\\n'\n",
    "for r in results:\n",
    "    for pname in r[0]:\n",
    "        out += (\"%4.2f\"%(r[0][pname]))+'\\t'\n",
    "    out += (\"%1.4f\"%(r[1]))+'\\n\\r'\n",
    "print(out)\n",
    "\n",
    "# saving the best results for later\n",
    "best_params['rbf'] = best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q1) [Answer the following]\n",
    "\n",
    "What do you observe when using RBF and polynomial kernels on this dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q1:**: they seem to achieve higher score figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.6)\n",
    "Report here the best SVM kernel and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "best_svm = SVC(kernel='rbf', C=100., gamma=0.1) # USE YOUR OPTIMAL PARAMETERS HERE (replace None)\n",
    "\n",
    "# Run the svm model on the whole training set\n",
    "best_svm.fit(X_train, y_train)\n",
    "score_training = best_svm.score(X_train, y_train)\n",
    "score_testing = best_svm.score(X_test, y_test)\n",
    "\n",
    "# Compute the errors\n",
    "# (error is 1 - svm.score)\n",
    "training_error = 1.0 - score_training # ADD YOUR CODE (replace None)\n",
    "test_error = 1.0 - score_testing # ADD YOUR CODE (replace None)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.7)\n",
    "\n",
    "Analyze how the gamma parameter (inversely proportional to standard deviation of Gaussian Kernel) impact the performances of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different values of gamma\n",
    "# use rbf kernel and C=1\n",
    "gamma_rbf = SVC(kernel='rbf')\n",
    "# Set gamma values\n",
    "gamma_values = np.logspace(-5,2,8)\n",
    "print(gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list, test_acc_list = [], []\n",
    "\n",
    "# ADD YOUR CODE TO TRAIN THE SVM MULTIPLE TIMES WITH THE DIFFERENT VALUES OF GAMMA\n",
    "# PLACE THE TRAIN AND TEST ACCURACY FOR EACH TEST IN THE TRAIN AND TEST ACCURACY LISTS\n",
    "for g in gamma_values:\n",
    "    gamma_rbf.set_params(**{'C':1, 'gamma':g})\n",
    "    gamma_rbf.fit(X_train, y_train)    \n",
    "    train_acc_list.append(gamma_rbf.score(X_train, y_train))\n",
    "    test_acc_list.append(gamma_rbf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(gamma_values, train_acc_list)\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('gamma')\n",
    "ax[0].set_ylabel('Train accuracy')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(gamma_values, test_acc_list)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('gamma')\n",
    "ax[1].set_ylabel('Test accuracy')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) More data\n",
    "Now let's do the same but using more data points for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.0)\n",
    "\n",
    "Choose a higher number of data points (e.g. x = 10000) for training data depending on your computing capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 600 # ADD YOUR CODE: adjust depending on the capabilities of your PC (replace None)\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True) # ADD YOUR CODE (replace None)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)\n",
    "\n",
    "# initialize support variables for boundaries visualization\n",
    "granularity = 25\n",
    "x_max = np.abs(X).max()\n",
    "x_range = np.linspace(-x_max, x_max, granularity)\n",
    "x_grid = np.stack(np.meshgrid(x_range, x_range, x_range)).reshape(3, -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.1)\n",
    "\n",
    "Let's try to use SVM with parameters obtained from the best model for $m_{training} =  10000$. Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the TO DO (C.Q1) cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "best_svm.fit(X_train, y_train)\n",
    "train_score = best_svm.score(X_train, y_train)\n",
    "test_score = best_svm.score(X_test, y_test)\n",
    "training_error = 1.0 - train_score\n",
    "test_error = 1.0 - test_score\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Boundaries Visualization\n",
    "\n",
    "Now let us plot the classification boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.0)\n",
    "\n",
    "Use the SVM to predict on the test set X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_test = best_svm.predict(X_test) # ADD YOUR CODE (replace None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constructed a grid of all possible combinations of input values, we now use it to extract the classification boundaries of the three classifiers by having them predict on each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_SVM_grid = best_svm.predict(x_grid)\n",
    "\n",
    "rbf_SVM_m = y_test == rbf_svm_test\n",
    "\n",
    "fig = plt.figure(figsize=(20,36))\n",
    "ax1 = fig.add_subplot(3, 1, 1, projection=\"3d\", title=\"RBF\")\n",
    "ax2 = fig.add_subplot(3, 1, 2, projection=\"3d\", title=\"POLY (degree=3)\")\n",
    "ax3 = fig.add_subplot(3, 1, 3, projection=\"3d\", title=\"LINEAR\")\n",
    "\n",
    "ax1.scatter(x_grid[:,0], x_grid[:,1], x_grid[:,2], c=rbf_SVM_grid, linewidth=0, marker=\"s\", alpha=.05,cmap='Accent')\n",
    "ax1.scatter(X_test[rbf_SVM_m,0], X_test[rbf_SVM_m,1], X_test[rbf_SVM_m,2], c=y_test[rbf_SVM_m], linewidth=.5, edgecolor=\"k\", marker=\".\",cmap='Accent')\n",
    "ax1.scatter(X_test[~rbf_SVM_m,0], X_test[~rbf_SVM_m,1], X_test[~rbf_SVM_m,2], c=y_test[~rbf_SVM_m], linewidth=1, edgecolor=\"r\", marker=\".\",cmap='Accent')\n",
    "ax1.set_xlim([-x_max, x_max])\n",
    "ax1.set_ylim([-x_max, x_max])\n",
    "ax1.set_zlim([-x_max, x_max])\n",
    "\n",
    "# using the polynomial kernel with the best parameters for it\n",
    "best_poly_svm = SVC(kernel='poly', degree=3, C=best_params['poly3']['C'], gamma=best_params['poly3']['gamma'])\n",
    "best_poly_svm.fit(X_train, y_train)\n",
    "poly_SVM_grid = best_poly_svm.predict(x_grid)\n",
    "poly_SVM_test = best_poly_svm.predict(X_test)\n",
    "poly_SVM_m = y_test == poly_SVM_test\n",
    "ax2.scatter(x_grid[:,0], x_grid[:,1], x_grid[:,2], c=poly_SVM_grid, linewidth=0, marker=\"s\", alpha=.05,cmap='Accent')\n",
    "ax2.scatter(X_test[poly_SVM_m,0], X_test[poly_SVM_m,1], X_test[poly_SVM_m,2], c=y_test[poly_SVM_m], linewidth=.5, edgecolor=\"k\", marker=\".\",cmap='Accent')\n",
    "ax2.scatter(X_test[~poly_SVM_m,0], X_test[~poly_SVM_m,1], X_test[~poly_SVM_m,2], c=y_test[~poly_SVM_m], linewidth=1, edgecolor=\"r\", marker=\".\",cmap='Accent')\n",
    "ax2.set_xlim([-x_max, x_max])\n",
    "ax2.set_ylim([-x_max, x_max])\n",
    "ax2.set_zlim([-x_max, x_max])\n",
    "\n",
    "# using the linear kernel with the best parameters for it\n",
    "best_linear_svm = SVC(kernel='linear', C=best_params['linear']['C'])\n",
    "best_linear_svm.fit(X_train, y_train)\n",
    "linear_SVM_grid = best_linear_svm.predict(x_grid)\n",
    "linear_SVM_test = best_linear_svm.predict(X_test)\n",
    "linear_SVM_m = y_test == linear_SVM_test\n",
    "ax3.scatter(x_grid[:,0], x_grid[:,1], x_grid[:,2], c=linear_SVM_grid, linewidth=0, marker=\"s\", alpha=.05,cmap='Accent')\n",
    "ax3.scatter(X_test[linear_SVM_m,0], X_test[linear_SVM_m,1], X_test[linear_SVM_m,2], c=y_test[linear_SVM_m], linewidth=.5, edgecolor=\"k\", marker=\".\",cmap='Accent')\n",
    "ax3.scatter(X_test[~linear_SVM_m,0], X_test[~linear_SVM_m,1], X_test[~linear_SVM_m,2], c=y_test[~linear_SVM_m], linewidth=1, edgecolor=\"r\", marker=\".\",cmap='Accent')\n",
    "ax3.set_xlim([-x_max, x_max])\n",
    "ax3.set_ylim([-x_max, x_max])\n",
    "ax3.set_zlim([-x_max, x_max])\n",
    "\n",
    "#ax2.view_init(elev=0, azim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q1) [Answer the following]**\n",
    "\n",
    "Compare and discuss the results from SVM with m=600 and with m=10000 (or whatever value you set) training data points. If you stopped the SVM, include such aspect in your comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q1:** when a smaller training set (m=600) is used, the number of misclassified samples is of course greater than in the case of a bigger training set (m=10000). However, using the suggested amount of noise (0.1),  while with m=600 the training error is less than the test error, as it is expected, in the case with m=10000 the training error is larger than the test error. This could be an indication of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.1)\n",
    "\n",
    "Plot the confusion matrix for the SVM classifier. The confusion matrix has one column for each predicted label and one row for each true label. \n",
    "It shows for each class in the corresponding row how many samples belonging to that class gets each possible output label. Notice that the diagonal contains the correctly classified samples, while the other cells correspond to errors. You can obtain it with the sklearn.metrics.confusion_matrix function (see the documentation). You can also print also the normalized confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) # for better aligned printing of confusion matrix use floatmode='fixed'\n",
    "\n",
    "u, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Labels and frequencies in test set: \", counts)\n",
    "\n",
    "confusion_SVM = skm.confusion_matrix(y_test, rbf_svm_test)\n",
    "print(\"\\n Confusion matrix SVM  \\n \\n\", confusion_SVM)\n",
    "print(\"\\n Confusion matrix SVM (normalized)   \\n \\n\", confusion_SVM /counts[:,None] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "    \n",
    "im = plt.imshow(confusion_SVM /counts[:,None], cmap=\"Blues\",interpolation='nearest')\n",
    "plt.xticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "plt.yticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "cm = confusion_SVM /counts[:,None]\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        ha=\"center\", va=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.colorbar(im, location='bottom')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q2) [Answer the following]\n",
    "\n",
    "Have a look at the confusion matrix and comment on the obtained accuracies. Why some classes have lower accuracies and others an higher one? Make some guesses on the possible causes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q2:** the confusion matrix seems to suggest that the classification algorithm encounters difficulties when it comes to correctly classify cloudy and rainy situations. This in a way makes sense since cloudy and rainy situations exhibit similar atmospheric conditions. This behaviour is also dependent on the amount of noise being introduced in the data: the more noise is added, the more \"confused\" the cloudy/rainy classification gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
